# -*- coding: utf-8 -*-
"""Copy of stress-efficientnetb0_evaluate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ptSGAZGAwxvyWMEZ_yCvLWl7SlMHwH9J
"""

import torch

from IPython.display import Image, clear_output  # to display images
# from utils.google_utils import gdrive_download  # to download models/datasets

# clear_output()
print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))

# Commented out IPython magic to ensure Python compatibility.
import json
import math
import os

import cv2
from PIL import Image
import numpy as np
import tensorflow as tf
from tensorflow import keras
from glob import glob
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import cohen_kappa_score, accuracy_score
import scipy
from tqdm import tqdm
import gc
from functools import partial
from sklearn import metrics
from collections import Counter
import json
import itertools
# from keras.applications.resnet50 import ResNet50
from keras.optimizers import Adam, RMSprop
from keras.callbacks import ReduceLROnPlateau
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
import efficientnet.tfkeras as efn 
# %matplotlib inline

SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)

"""# Loading & Preprocessing"""

image_dir = '/content/drive/MyDrive/pkm/stress detection/dataset/wawancara/spectogram_pcen_15w_cropped'
df_dir = '/content/drive/MyDrive/pkm/stress detection/dataset/wawancara/df_wawancara.xlsx'
df_test_dir = '/content/drive/MyDrive/pkm/stress detection/dataset/wawancara'
model_dir = '/content/drive/MyDrive/pkm/stress detection/dataset/wawancara/efficientnetb0_5w_2*.best*'
normal_list = glob(f'{image_dir}/normal_*/*')
stress_list = glob(f'{image_dir}/stress_*/*')

df_label = pd.read_excel(df_dir)
df_label = df_label[~df_label['skor_dass'].isna()]
df_label['status'] = df_label.skor_dass.apply(lambda p: 'normal' if p<=14 else 'stress')
df_label['id_file'] = df_label.apply(lambda x:x['status']+'_'+x['nama_file'].split('.')[0],axis = 1)
df_label.sort_values('id_file')


df_file = pd.DataFrame(normal_list+stress_list,columns = ['filename'])
df_file['id_file'] = df_file.filename.apply(lambda p: p.split('/')[-2].split('.')[0])

df_file = df_file.merge(df_label,on='id_file')

#Transfer 'jpg' images to an array IMG
def Dataset_loader(image_list):
    IMG = []
    read = lambda imname: np.asarray(Image.open(imname).convert("RGB"))
    for IMAGE_NAME in tqdm(image_list):
      img = read(IMAGE_NAME)
      img = cv2.resize(img, (800,160))
#             img = segment_image(img)
      IMG.append(np.array(img))
    return np.array(IMG)

X = Dataset_loader(list(df_file.filename))
y = df_file.status.apply(lambda p: 0 if p=='normal' else 1)
y = tf.keras.utils.to_categorical(y, num_classes= 2)

"""# Create Label"""

import numpy as np
from sklearn.model_selection import GroupKFold
groups = df_file.id_file
group_kfold = GroupKFold(n_splits=5)
group_kfold.get_n_splits(X, y, groups)

print(group_kfold)

model_list = glob(model_dir)

model_list.sort()
model_list

from tensorflow.keras.utils import plot_model
model = tf.keras.models.load_model(model_list[0])
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

from sklearn.metrics import f1_score,confusion_matrix,accuracy_score,recall_score,precision_score



# val_generator.reset()
i = 0
acc = []
recall = []
precision = []
f1 = []
for train_index, test_index in group_kfold.split(X, y, groups):
  i+=1
  model = tf.keras.models.load_model(model_list[i-1])
  Y_val_pred = model.predict(X[test_index])
  print('K-Fold: ',i)
  print("acc",accuracy_score(np.argmax(y[test_index], axis=1), np.argmax(Y_val_pred, axis=1)))
  print("recall",recall_score(np.argmax(y[test_index], axis=1), np.argmax(Y_val_pred, axis=1)))
  print("precision",precision_score(np.argmax(y[test_index], axis=1), np.argmax(Y_val_pred, axis=1)))
  print("f1",f1_score(np.argmax(y[test_index], axis=1), np.argmax(Y_val_pred, axis=1)))
  print('')
  acc += [accuracy_score(np.argmax(y[test_index], axis=1), np.argmax(Y_val_pred, axis=1))]
  recall += [recall_score(np.argmax(y[test_index], axis=1), np.argmax(Y_val_pred, axis=1))]
  precision+= [precision_score(np.argmax(y[test_index], axis=1), np.argmax(Y_val_pred, axis=1))]
  f1+= [f1_score(np.argmax(y[test_index], axis=1), np.argmax(Y_val_pred, axis=1))]
  df_test =df_file.loc[test_index].copy()
  df_test.reset_index
  df_test['label'] = df_test.skor_dass.apply(lambda p: 0 if p<=14 else 1)
  df_test['target'] = np.argmax(Y_val_pred,axis=1)
  df_test.to_csv(f'{df_test_dir}/df_test_5w_{i}.csv')

print(np.mean(acc),np.std(acc))
print(np.mean(recall),np.std(recall))
print(np.mean(precision),np.std(precision))
print(np.mean(f1),np.std(f1))

df_result = []
for i in glob(f'{df_test_dir}/df_test_5w_*.csv'):
  df_result+= [pd.read_csv(i)]

df_result = pd.concat(df_result)

df_result['frame'] = df_result.filename.apply(lambda p: (p.split('/')[-1][:-4]).split('('))
df_result['record'] = df_result.filename.apply(lambda p: ((p.split('/')[-2])))

df_result2 = df_result.groupby(by = 'record').mean()

print("acc",accuracy_score(df_result2.label,df_result2.target.apply(lambda p:1 if p>0.5 else 0)))
print("recall",recall_score(df_result2.label,df_result2.target.apply(lambda p:1 if p>0.5 else 0)))
print("precision",precision_score(df_result2.label,df_result2.target.apply(lambda p:1 if p>0.5 else 0)))
print("f1",f1_score(df_result2.label,df_result2.target.apply(lambda p:1 if p>0.5 else 0)))